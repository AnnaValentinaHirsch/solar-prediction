{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mitmischer/solar-prediction/blob/main/notebook_3_univariate_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_fQm6m_Oii_V",
      "metadata": {
        "id": "_fQm6m_Oii_V"
      },
      "source": [
        "# Package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9IcV3WEJ3Hp4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IcV3WEJ3Hp4",
        "outputId": "4afb2001-aebc-4904-f463-cd89739dc195"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CSkEv8ErdCEl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSkEv8ErdCEl",
        "outputId": "ed920a4a-667e-444b-a986-870fe40e8127"
      },
      "outputs": [],
      "source": [
        "!pip install armagarch pyextremes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NSHL72-qakUs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSHL72-qakUs",
        "outputId": "cbd878e9-21f6-487b-8edb-43aa346e21b5"
      },
      "outputs": [],
      "source": [
        "!pip install gluonts armagarch statsmodels statsforecast s3fs datasetsforecast GPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NZKy5GREdmyK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NZKy5GREdmyK",
        "outputId": "acf1a5d8-b4c6-4b26-be9b-be9ab77aa715"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/Nixtla/neuralforecast.git@main git+https://github.com/RJT1990/pyflux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gMDhZWlTaokN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMDhZWlTaokN",
        "outputId": "46ba6f96-3c67-4316-bf94-7e1309acb542"
      },
      "outputs": [],
      "source": [
        "!pip install darts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U8SsNwEvipI0",
      "metadata": {
        "id": "U8SsNwEvipI0"
      },
      "source": [
        "# Notebook parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iQcUnSu6ipYn",
      "metadata": {
        "id": "iQcUnSu6ipYn"
      },
      "outputs": [],
      "source": [
        "# if False, cells that take a lot of time are not run.\n",
        "# For production and the actual results, use slow=True!\n",
        "slow = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zr7ckL8uw3Ub",
      "metadata": {
        "id": "zr7ckL8uw3Ub"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-DMHewklw6WR",
      "metadata": {
        "id": "-DMHewklw6WR"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pRJXPPMFxRHR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRJXPPMFxRHR",
        "outputId": "479d3612-d1d5-49c5-a146-9a179bc922aa"
      },
      "outputs": [],
      "source": [
        "splits = {'train': 'train_ts.csv', 'validation': 'val_ts.csv', 'test': 'test_ts.csv'}\n",
        "df_train = pd.read_csv(\"hf://datasets/Creatorin/solarpower/\" + splits[\"train\"])\n",
        "df_validation = pd.read_csv(\"hf://datasets/Creatorin/solarpower/\" + splits[\"validation\"])\n",
        "df_test = pd.read_csv(\"hf://datasets/Creatorin/solarpower/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sAveNgvvxUSq",
      "metadata": {
        "id": "sAveNgvvxUSq"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_train, df_validation, df_test])\n",
        "ts = df[\"Leistung\"]\n",
        "ts.index = pd.to_datetime(ts.index, utc=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OTcGhdsXXbs7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTcGhdsXXbs7",
        "outputId": "ffbeb0e9-0a74-488c-b2d1-90c5acb869b6"
      },
      "outputs": [],
      "source": [
        "ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B_vwXZxnXHZc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "B_vwXZxnXHZc",
        "outputId": "24b20d8f-656c-43fe-fa54-d8633b34e2ac"
      },
      "outputs": [],
      "source": [
        "# see https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data\n",
        "pv = pd.pivot_table(df, index=ts.index.month, columns=ts.index.year,\n",
        "                    values='Leistung', aggfunc='sum')\n",
        "pv.plot(cmap=\"Greys\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NDgeubYhYDW0",
      "metadata": {
        "id": "NDgeubYhYDW0"
      },
      "source": [
        "The increasing monthly trend is clearly visible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M_qu9goMX6oZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_qu9goMX6oZ",
        "outputId": "a165b7c4-88ce-4536-a4cf-b9aa88c1b339"
      },
      "outputs": [],
      "source": [
        "# Those values need imputation!\n",
        "pd.date_range(ts.index.min(), ts.index.max(), freq='15Min').difference(ts.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J6a8cfItXQ1N",
      "metadata": {
        "id": "J6a8cfItXQ1N"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1uz8OqmmzoO8",
      "metadata": {
        "id": "1uz8OqmmzoO8"
      },
      "outputs": [],
      "source": [
        "# Interpolation\n",
        "ts = ts.interpolate(method=\"time\")\n",
        "# Only now can we infer a frequency.\n",
        "ts = ts.asfreq(pd.infer_freq(ts.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BVRqC_HAzrwF",
      "metadata": {
        "id": "BVRqC_HAzrwF"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "avg_train, dev_train = ts.mean(), ts.std()\n",
        "ts_series = (ts - avg_train)/dev_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QdzqgaQ_ztGO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "QdzqgaQ_ztGO",
        "outputId": "9c1998a0-0bbb-49d7-bb94-a039a0a32adb"
      },
      "outputs": [],
      "source": [
        "# Remove trend\n",
        "ts_series = ts_series.diff().dropna()\n",
        "ts_series.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cEDlyFxbz0JZ",
      "metadata": {
        "id": "cEDlyFxbz0JZ"
      },
      "outputs": [],
      "source": [
        "# remove increasing volatility\n",
        "\n",
        "def remove_volatility(ts):\n",
        "    annual_volatility = ts.groupby(ts.index.year).std()\n",
        "    annual_vol_per_day = ts.index.map(lambda d: annual_volatility.loc[d.year])\n",
        "    ts_corrected_variance = ts/annual_vol_per_day\n",
        "    monthly_mean = ts_corrected_variance.groupby(ts_corrected_variance.index.month).mean()\n",
        "    monthly_mean_per_day = ts_corrected_variance.index.map(lambda d: monthly_mean.loc[d.month])\n",
        "    ts_corrected_variance= ts_corrected_variance - monthly_mean_per_day\n",
        "    ts_corrected_variance= ts_corrected_variance[~np.isnan(ts_corrected_variance)]\n",
        "    return ts_corrected_variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3QYoJIHIDGuo",
      "metadata": {
        "id": "3QYoJIHIDGuo"
      },
      "outputs": [],
      "source": [
        "annual_volatility = ts.groupby(ts.index.year).std()\n",
        "annual_vol_per_day = ts.index.map(lambda d: annual_volatility.loc[d.year])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7R7gLz5rY7Kh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "7R7gLz5rY7Kh",
        "outputId": "db02931a-9fc8-436c-e03d-6f4ab6b05134"
      },
      "outputs": [],
      "source": [
        "annual_volatility[0:10].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_QzZwr7KDNuY",
      "metadata": {
        "id": "_QzZwr7KDNuY"
      },
      "source": [
        "Removing the increasing volatility will yield a perfectly stationary time series. One might argue that this is information leakage, but the annual volatility follows a linear trend and is inferrable.\n",
        "The values for 2024 are removed from the plot as the year 2024 is not complete (and has a lower volatility)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRaqd0xBz5Uo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nRaqd0xBz5Uo",
        "outputId": "b8ff3b15-d035-4b81-a124-192c5a08f761"
      },
      "outputs": [],
      "source": [
        "ts = remove_volatility(ts_series)\n",
        "ts.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c81ebe2-0a6e-4515-82d4-70eefb852582",
      "metadata": {
        "id": "3c81ebe2-0a6e-4515-82d4-70eefb852582"
      },
      "source": [
        "# Spectral Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7evQeNTqEq7O",
      "metadata": {
        "id": "7evQeNTqEq7O"
      },
      "outputs": [],
      "source": [
        "# spectral analysis\n",
        "from scipy import signal\n",
        "from scipy.signal import periodogram as periodogram_f\n",
        "from scipy.fft import fftfreq, fftshift\n",
        "from scipy.fft import fft, ifft, fft2, ifft2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rMbzViunLRI2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "rMbzViunLRI2",
        "outputId": "cb7b30ee-24c8-4734-e9da-77198d16c5e6"
      },
      "outputs": [],
      "source": [
        "# looks (and should be!) similar to the \"manual\" calculation above.\n",
        "# Note that in the manual calculation, we get a symmetric graph. That's to be expected (check out the videos).\n",
        "dt = 15*60\n",
        "rate = 1/dt\n",
        "frequencies, periodogram = periodogram_f(np.asarray(ts), fs=rate, window=\"hamming\")\n",
        "plt.plot(fftshift(frequencies), fftshift(periodogram))\n",
        "plt.xlim(-0.00002, 0.0001*4)\n",
        "plt.ylim(0, 10000/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlD_n6V1Gs_l",
      "metadata": {
        "id": "VlD_n6V1Gs_l"
      },
      "source": [
        "We see some clear peaks corresponding to strong frequencies in the signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TaGUqWiAGoHF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "TaGUqWiAGoHF",
        "outputId": "81e76151-1ae4-4bda-e913-31e5b596564d"
      },
      "outputs": [],
      "source": [
        "periodogram_as_series = pd.Series(fftshift(periodogram), index=fftshift(frequencies))\n",
        "periodogram_as_series = periodogram_as_series[periodogram_as_series.index > 0]\n",
        "# convert index from frequencies to periods and convert the periods to hours\n",
        "# TODO: is the calculation to hours correct (note that  we already specified the sampling rate during the fft!)?\n",
        "periodogram_as_series.index = (1/periodogram_as_series.index)/3600\n",
        "plt.plot(periodogram_as_series)\n",
        "plt.xlim(0,100000/3600/2)\n",
        "plt.ylim(0, 5000*10/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8EmyHMsnL7iU",
      "metadata": {
        "id": "8EmyHMsnL7iU"
      },
      "source": [
        "The frequencies can be converted to time intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d9996f-2114-465c-bf86-74eb46a108fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "10d9996f-2114-465c-bf86-74eb46a108fb",
        "outputId": "eee1fe03-dd57-42b9-f720-c7e38d44e7df"
      },
      "outputs": [],
      "source": [
        "# yet another way to calculate the FFT, from the \"denoising\" video.\n",
        "# That'll allow us to drop frequencies later.\n",
        "n = len(ts)\n",
        "fhat = np.fft.fft(ts, n)\n",
        "PSD = fhat*np.conj(fhat)/n\n",
        "freq = (1/(dt*n))*np.arange(n)\n",
        "L= np.arange(1, np.floor(n/2), dtype=\"int\")\n",
        "plt.plot(freq[L], PSD[L])\n",
        "plt.xlim(-0.00002, 0.0001*4)\n",
        "plt.ylim(0, 10000/(1500*4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b75c6bf-4c2e-4329-ab46-7911dc4fbce4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "0b75c6bf-4c2e-4329-ab46-7911dc4fbce4",
        "outputId": "d6fc8e37-06ab-4ca9-bff9-c82379ea6de3"
      },
      "outputs": [],
      "source": [
        "# Now decide on the frequencies to cut off.\n",
        "plt.plot(freq, PSD)\n",
        "plt.ylim(1e-2,1e5)\n",
        "plt.xlim(0,max(freq)/2)\n",
        "plt.axhline(y=1e-1, color=\"r\")\n",
        "plt.semilogy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NeoDwNEnHLhw",
      "metadata": {
        "id": "NeoDwNEnHLhw"
      },
      "source": [
        "We want to keep the strong frequencies in the signal and remove the other ones (which make the signal noisy).\n",
        "Visually, a cutoff of 1 drops a lot of less important frequencies while keeping ~80% of the major ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fce48a-95e8-463b-a1dc-f6458d7de9e8",
      "metadata": {
        "id": "d4fce48a-95e8-463b-a1dc-f6458d7de9e8"
      },
      "outputs": [],
      "source": [
        "indices = PSD > 1e-1\n",
        "# Filter and reconstruct the signal on the retained frequencies (reverse fourier transform)\n",
        "PSDclean = PSD*indices\n",
        "fhat = indices*fhat\n",
        "ffilt = np.fft.ifft(fhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PhqflfmFl9ks",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhqflfmFl9ks",
        "outputId": "1f452e55-4f37-4076-e0bb-8dc84f19f701"
      },
      "outputs": [],
      "source": [
        "# number of surviving frequencies\n",
        "np.sum(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad1312d-823b-4b7f-8614-ab59e412a65a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "6ad1312d-823b-4b7f-8614-ab59e412a65a",
        "outputId": "bc19961c-d64b-4310-fb1b-ffcffd2da970"
      },
      "outputs": [],
      "source": [
        "plt.plot(freq, PSDclean)\n",
        "plt.ylim(1e-2,1e5)\n",
        "plt.xlim(0,max(freq)/2)\n",
        "plt.axhline(y=1e0, color=\"r\")\n",
        "plt.semilogy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6acb68-7925-4894-8a56-b0c4630adfb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "aa6acb68-7925-4894-8a56-b0c4630adfb1",
        "outputId": "0bd1b619-d064-4396-f429-8c1af5b11661"
      },
      "outputs": [],
      "source": [
        "ts[1:200].plot()\n",
        "plt.ylim(-4,4)\n",
        "pd.Series(ffilt[1:200], ts.index[1:200]).plot()\n",
        "plt.ylim(-1,1)\n",
        "plt.legend([\"original\", \"after fft\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3riHjejYNwlj",
      "metadata": {
        "id": "3riHjejYNwlj"
      },
      "source": [
        "Generally, in many of the settings we investigated, FFT had problems with the zero production at night.\n",
        "You can play around with the parameter in\n",
        "`indices = PSD > X`\n",
        "to obtain the comparisons in the presentation.\n",
        "The FT model later in the notebook will work better, maybe by a better choice of parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aMGcWv1xwqWF",
      "metadata": {
        "id": "aMGcWv1xwqWF"
      },
      "source": [
        "# Gaussian Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yOUvJKd1j9mv",
      "metadata": {
        "id": "yOUvJKd1j9mv"
      },
      "source": [
        "We employ Gaussian processes as a filtering / preprocessing method, not for forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rkIZS0vyniNV",
      "metadata": {
        "id": "rkIZS0vyniNV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process.kernels import ExpSineSquared, RBF\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.models import GaussianProcessFilter\n",
        "from darts.utils import timeseries_generation as tg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JMgV81z7hjhy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "JMgV81z7hjhy",
        "outputId": "9c21343a-8fc5-4b30-bcf7-3715b9f5ed8d"
      },
      "outputs": [],
      "source": [
        "# The Gaussian Process will *not* work on the hourly data, but only\n",
        "# on the 15-minute data (!). Therefore, we need to have this in the meantime.\n",
        "solar_ts=pd.read_csv(\"/content/drive/MyDrive/energy_charts.csv\", sep=\",\", header=0)#date_format='%Y-%m-%d %H:%M:%S%z')#parse_dates={\"date\": [\"Datum\"]})\n",
        "solar_ts['Datum']=pd.to_datetime(solar_ts['Datum'], format='%Y-%m-%dT%H:%M%z', utc=True)\n",
        "solar_ts=solar_ts.set_index(keys=\"Datum\",drop=True)\n",
        "solar_ts.plot()\n",
        "# This add NaN as value for the missing indices, we can impute this later.\n",
        "solar_ts = solar_ts.resample(\"15Min\").first()\n",
        "# As only a few values need imputation, so the choice of the imputation algorithm does not matter much.\n",
        "solar_ts = solar_ts.interpolate(method=\"time\")\n",
        "# Only now can we infer a frequency.\n",
        "solar_ts=solar_ts.asfreq(pd.infer_freq(solar_ts.index))\n",
        "solar_ts_series = solar_ts.Leistung\n",
        "# Normalize\n",
        "train_size = int(len(solar_ts_series) * 0.9)\n",
        "ts_train = solar_ts_series[:train_size]\n",
        "ts_test = solar_ts_series[train_size:]\n",
        "\n",
        "# we cannot normalize over the test data as we don't have that yet!\n",
        "avg, dev = ts_train.mean(), ts_train.std()\n",
        "solar_ts_series = (solar_ts_series - avg)/dev\n",
        "solar_ts_series.plot()\n",
        "# Remove trend (TODO: compare with the approach in the Fourier series video, where they also detrend?)\n",
        "solar_ts_series = solar_ts_series.diff().dropna()\n",
        "solar_ts_series.plot()\n",
        "# remove increasing volatility - or (TODO: use a (G)ARCH here).\n",
        "annual_volatility = solar_ts_series.groupby(solar_ts_series.index.year).std()\n",
        "annual_vol_per_day = solar_ts_series.index.map(lambda d: annual_volatility.loc[d.year])\n",
        "solar_ts_series_corrected_variance = solar_ts_series/annual_vol_per_day\n",
        "solar_ts_series_corrected_variance.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "im6-_uROh3Lc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im6-_uROh3Lc",
        "outputId": "e8b19360-304b-4edf-9140-a779538d8cf6"
      },
      "outputs": [],
      "source": [
        "from darts import TimeSeries\n",
        "\n",
        "train_size = np.sum(solar_ts_series_corrected_variance.index < pd.Timestamp('2020-01-01 00:00:00+0000', tz='UTC'))\n",
        "\n",
        "ts_train = solar_ts_series_corrected_variance[:train_size]\n",
        "ts_test = solar_ts_series_corrected_variance[train_size:]\n",
        "\n",
        "darts_ts_train = TimeSeries.from_series(ts_train)\n",
        "darts_ts_test = TimeSeries.from_series(ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caAHfdtPwwEE",
      "metadata": {
        "id": "caAHfdtPwwEE"
      },
      "outputs": [],
      "source": [
        "NOISE_DISTANCE = 0.4\n",
        "SAMPLE_SIZE = 200\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aSccS9ENHyou",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "aSccS9ENHyou",
        "outputId": "60a44da9-8824-4ac6-8ab3-abafef06c515"
      },
      "outputs": [],
      "source": [
        "kernel = ExpSineSquared()\n",
        "\n",
        "gpf = GaussianProcessFilter(\n",
        "    kernel=kernel, alpha=NOISE_DISTANCE / 2, n_restarts_optimizer=100\n",
        ")\n",
        "\n",
        "\n",
        "filtered_x_samples = gpf.filter(darts_ts_train[:300], num_samples=10)\n",
        "\n",
        "plt.figure(figsize=[12, 8])\n",
        "darts_ts_train[:300].plot(color=\"black\", label=\"Original solar data\")\n",
        "filtered_x_samples.plot(color=\"blue\", label=\"Confidence interval of filtered solar data\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yBTgxjhESjab",
      "metadata": {
        "id": "yBTgxjhESjab"
      },
      "source": [
        "Given the general energy curve shape (sinusoidal) and the increasing trend in data volatility, ExpSineSquared is the obvious first choice, but its performance is not great.\n",
        "The results look similar to FFT (will be presented later in this notebook), but there is big problem - the resulting output is just repetitive (each 1-day period is identical).\n",
        "We've seen this for the FFT, too, if too few frequencies were retained. Even for the very small training set, the cell takes a lot of time to execute, so the effect of raising num_samples could not be investigated further. Also, the runtimes are unbearingly slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2_5qY-uTqfx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "f2_5qY-uTqfx",
        "outputId": "ab2a4188-100e-454f-902b-1b1ade5bc174"
      },
      "outputs": [],
      "source": [
        "kernel = RBF()\n",
        "\n",
        "gpf = GaussianProcessFilter(\n",
        "    kernel=kernel, alpha=NOISE_DISTANCE / 2, n_restarts_optimizer=100\n",
        ")\n",
        "\n",
        "filtered_x_samples = gpf.filter(darts_ts_train[:600], num_samples=10) # num_samples were 100\n",
        "\n",
        "plt.figure(figsize=[12, 8])\n",
        "darts_ts_train[:600].plot(color=\"black\", label=\"Original solar data\")\n",
        "filtered_x_samples.plot(color=\"blue\", label=\"Confidence interval of filtered solar data\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SRiMRWzuVHRr",
      "metadata": {
        "id": "SRiMRWzuVHRr"
      },
      "source": [
        "The RBF kernel performs much better here, capturing the general peak height. Still, as with FFT, there are problems at night time.\n",
        "One can safely reduce num_samples to 50 without high increases in error, still the high execution times make applying the filter on the whole dataset unfeasible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kywfuwuJHkso",
      "metadata": {
        "id": "kywfuwuJHkso"
      },
      "source": [
        "# Anomaly Detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H3PbUzG9HnUr",
      "metadata": {
        "id": "H3PbUzG9HnUr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r5Z5Am-eIOPE",
      "metadata": {
        "id": "r5Z5Am-eIOPE"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 288\n",
        "\n",
        "# Generated training sequences for use in the model:\n",
        "# rolling windows of size TIME_STEPS\n",
        "def create_sequences(values, time_steps):\n",
        "    output = []\n",
        "    for i in range(len(values) - time_steps + 1):\n",
        "        output.append(values[i : (i + time_steps)])\n",
        "    return np.stack(output)\n",
        "\n",
        "\n",
        "def prepare_data(x, sample_type, time_steps):\n",
        "    # normalize x (=train or test data) and create windows.\n",
        "    x_mean = x.mean()\n",
        "    x_std = x.std()\n",
        "    df_training_value = (x - x_mean) / x_std\n",
        "    print(f\"Number of {sample_type} samples:\", len(x))\n",
        "\n",
        "    x_ret = create_sequences(df_training_value.values, time_steps)\n",
        "    print(\"{sample_type} input shape: \", x_ret.shape)\n",
        "    return x_ret\n",
        "\n",
        "def AE_anomaly_detection(simple, x_train, x_test, time_steps):\n",
        "    x_train = prepare_data(x_train, \"training\", time_steps)\n",
        "\n",
        "    if simple:\n",
        "      model = keras.Sequential(\n",
        "          [\n",
        "            layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
        "            layers.Conv1D(\n",
        "                filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Dropout(rate=0.2),\n",
        "            layers.Conv1D(\n",
        "                filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Conv1DTranspose(\n",
        "                filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Dropout(rate=0.2),\n",
        "            layers.Conv1DTranspose(\n",
        "                filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "            ),\n",
        "            layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
        "          ]\n",
        "      )\n",
        "    else:\n",
        "      model = keras.Sequential(\n",
        "          [\n",
        "            layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2])),\n",
        "            layers.Dropout(rate=0.2),\n",
        "            layers.RepeatVector(x_train.shape[1]),\n",
        "            layers.LSTM(128, return_sequences=True),\n",
        "            layers.Dropout(rate=0.2),\n",
        "            layers.TimeDistributed(layers.Dense(x_train.shape[2])),\n",
        "          ]\n",
        "      )\n",
        "\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "    history = model.fit(\n",
        "    x_train,\n",
        "    x_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
        "       ],\n",
        "    )\n",
        "\n",
        "    ## Get train MAE loss, compute reconstruction error threshold.\n",
        "    x_train_pred = model.predict(x_train)\n",
        "    train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
        "\n",
        "    plt.hist(train_mae_loss, bins=50)\n",
        "    plt.xlabel(\"Train MAE loss\")\n",
        "    plt.ylabel(\"No of samples\")\n",
        "    plt.show()\n",
        "\n",
        "    # Get reconstruction loss threshold.\n",
        "    # Here, we use 90% of the maximum training threshold, but one could\n",
        "    # experiment with this value!\n",
        "    threshold = np.max(train_mae_loss)*0.9\n",
        "    print(\"Reconstruction error threshold: \", threshold)\n",
        "\n",
        "\n",
        "    x_test = prepare_data(x_test, \"test\", time_steps)\n",
        "\n",
        "    ## Get test MAE loss, determine anomalous slices.\n",
        "    x_test_pred = model.predict(x_test)\n",
        "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
        "    test_mae_loss = test_mae_loss.reshape((-1))\n",
        "\n",
        "    plt.hist(test_mae_loss, bins=50)\n",
        "    plt.xlabel(\"test MAE loss\")\n",
        "    plt.ylabel(\"No of samples\")\n",
        "    plt.show()\n",
        "\n",
        "    # Detect all the anomalous slices.\n",
        "    anomalies = test_mae_loss > threshold\n",
        "    print(anomalies)\n",
        "    print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "    return anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebC18C5wIQBQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ebC18C5wIQBQ",
        "outputId": "3daeb3c4-f40e-4547-96a9-df12adf66c8e"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 72 # window slice size\n",
        "# this parameter determines the train-test split\n",
        "# generally, also with just very little training data, anomaly detection looks\n",
        "# reasonable.\n",
        "train_end_idx=5000\n",
        "\n",
        "test_part = ts[(train_end_idx+1):]\n",
        "\n",
        "# Change simple to False to use an LSTM instead of a simple CNN.\n",
        "anomalies = AE_anomaly_detection(True, ts.iloc[2000:train_end_idx].to_frame(),\n",
        "                                 ts.iloc[(train_end_idx+1):].to_frame(),\n",
        "                                 time_steps=TIME_STEPS)\n",
        "\n",
        "anomalous_data_indices = []\n",
        "for data_idx in range(TIME_STEPS - 1, len(test_part) - TIME_STEPS + 1):\n",
        "    # get all windows where the sample is in.\n",
        "    sample_anomalies = anomalies[data_idx - TIME_STEPS + 1 : data_idx]\n",
        "    # a sample is anomalous if it is in more than 90% anomalous windows\n",
        "    if np.mean(sample_anomalies) > 0.9:\n",
        "        anomalous_data_indices.append(data_idx)\n",
        "abnormal_subset = test_part.iloc[anomalous_data_indices]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ts.plot(legend=False, ax=ax)\n",
        "if len(abnormal_subset)>0:\n",
        "  abnormal_subset.plot(legend=False, ax=ax, color=\"r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lrYTOtfBSMQ4",
      "metadata": {
        "id": "lrYTOtfBSMQ4"
      },
      "source": [
        "The histograms show the reconstruction errors on the training and test samples, respectively. The cutoff is set to 90% of the maximum reconstruction error on the training samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ls9IX98PUZ8V",
      "metadata": {
        "id": "ls9IX98PUZ8V"
      },
      "source": [
        "Note that the first 2000 time points were skipped as this yields a more regular histogram (without a heavy right tail).\n",
        "In the original series (with a frequency of 15 minutes), the histogram was regular without this preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DiQrKOLTZNSz",
      "metadata": {
        "id": "DiQrKOLTZNSz"
      },
      "source": [
        "Play around with the parameter simple to select either the CNN or the LSTM and\n",
        "the parameter train_end_idx.\n",
        "Only samples in the test set are evaluated for being anomalous, but we could apply the test on the training data, too.\n",
        "Note that for the plots in the presentation, the time series with increasing volatility was used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cgb9YbZ-P27V",
      "metadata": {
        "id": "Cgb9YbZ-P27V"
      },
      "source": [
        "# NBEATS (Darts), (works on the 1h data), *should* be good to go, needs testing on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0uWL2aLNP5KC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uWL2aLNP5KC",
        "outputId": "f8fb6abb-68b0-4ee7-cb09-48cabf1fba4a"
      },
      "outputs": [],
      "source": [
        "from darts import TimeSeries, concatenate\n",
        "from darts.utils.callbacks import TFMProgressBar\n",
        "from darts.models import NBEATSModel\n",
        "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
        "from darts.metrics import mape, r2_score\n",
        "from darts.datasets import EnergyDataset\n",
        "from darts import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C-Ynmm6nQY7V",
      "metadata": {
        "id": "C-Ynmm6nQY7V"
      },
      "outputs": [],
      "source": [
        "def display_forecast(pred_series, ts_transformed, forecast_type, start_date=None):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    if start_date:\n",
        "        ts_transformed = ts_transformed.drop_before(start_date)\n",
        "    ts_transformed.univariate_component(0).plot(label=\"actual\")\n",
        "    pred_series.plot(label=(\"historic \" + forecast_type + \" forecasts\"))\n",
        "    plt.title(\n",
        "        \"R2: {}\".format(r2_score(ts_transformed.univariate_component(0), pred_series))\n",
        "    )\n",
        "    plt.legend()\n",
        "\n",
        "def generate_torch_kwargs():\n",
        "    # run torch models on CPU, and disable progress bars for all model stages except training.\n",
        "    return {\n",
        "        \"pl_trainer_kwargs\": {\n",
        "            \"accelerator\": \"cpu\",\n",
        "            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kFVX7HlQbaC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "0kFVX7HlQbaC",
        "outputId": "f1602959-82f4-44e5-a69d-2bfde28c3f71"
      },
      "outputs": [],
      "source": [
        "scaler = Scaler()\n",
        "\n",
        "train_size = np.sum(ts.index < pd.Timestamp('2020-01-01 00:00:00+0000', tz='UTC'))\n",
        "ts_train = ts[:train_size]\n",
        "ts_test = ts[train_size:]\n",
        "\n",
        "# data is not normal yet!\n",
        "darts_ts_train = scaler.fit_transform(TimeSeries.from_series(ts_train))\n",
        "darts_ts_test = scaler.transform(TimeSeries.from_series(ts_test))\n",
        "darts_ts_full = scaler.transform(TimeSeries.from_series(ts))\n",
        "\n",
        "darts_ts_train.plot(label=\"training\")\n",
        "darts_ts_test.plot(label=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08T5vQ5QS_sN",
      "metadata": {
        "id": "08T5vQ5QS_sN"
      },
      "outputs": [],
      "source": [
        "model_name = \"nbeats_run\"\n",
        "model_nbeats = NBEATSModel(\n",
        "    input_chunk_length=30,\n",
        "    output_chunk_length=7,\n",
        "    generic_architecture=True,\n",
        "    num_stacks=10,\n",
        "    num_blocks=1,\n",
        "    num_layers=4,\n",
        "    layer_widths=512,\n",
        "    n_epochs=100,\n",
        "    nr_epochs_val_period=1,\n",
        "    batch_size=800,\n",
        "    random_state=42,\n",
        "    model_name=model_name,\n",
        "    save_checkpoints=True,\n",
        "    force_reset=True,\n",
        "    **generate_torch_kwargs(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nkpT-YrtTHZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "1ce5f0d9e6784d21a28f24ca525e54ac",
            "2c22c2a62565464da2e894e12d1e0366",
            "21e9ada380294ce1a67591120c498299",
            "32d505faf0f4482894a9b509e77b8c7b",
            "276a62e739b146379c0175483ace9961",
            "019a2b2407c44e698ca2a151c50b7228",
            "cfe5f80988e445aab74b86d439fc92d3",
            "07ea67f382ee4cec85f57d91abebcf8c",
            "6b0688e0fa6a4d6cbc57ed33d95cfd99",
            "cbfef429daf44f94b4a4dd07ea6d6058",
            "bba4155f11b441fe9b5fe53671e3e466"
          ]
        },
        "id": "nkpT-YrtTHZ-",
        "outputId": "868d450e-06b5-4961-9e13-742aac7d61e2"
      },
      "outputs": [],
      "source": [
        "# be sure to activate gpu if you have one (three cells above this one)\n",
        "if slow:\n",
        "  model_nbeats.n_epochs = 100\n",
        "  model_nbeats.fit(darts_ts_train, val_series=darts_ts_test)\n",
        "else:\n",
        "  model_nbeats.n_epochs = 20\n",
        "  model_nbeats.fit(darts_ts_train[0:500], val_series=darts_ts_test[0:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DwTyWRp3Tpfn",
      "metadata": {
        "id": "DwTyWRp3Tpfn"
      },
      "outputs": [],
      "source": [
        "model_nbeats = NBEATSModel.load_from_checkpoint(model_name=model_name, best=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aIi5Vp-gU_-h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "aIi5Vp-gU_-h",
        "outputId": "b9860d6e-342d-4914-9df1-5b8f0c9f47c4"
      },
      "outputs": [],
      "source": [
        "pred_series = model_nbeats.historical_forecasts(\n",
        "    darts_ts_full,\n",
        "    start=darts_ts_test.start_time(),\n",
        "    forecast_horizon=7,\n",
        "    stride=7,\n",
        "    last_points_only=False,\n",
        "    retrain=False,\n",
        "    verbose=True,\n",
        ")\n",
        "pred_series = concatenate(pred_series)\n",
        "\n",
        "display_forecast(\n",
        "    pred_series,\n",
        "    darts_ts_full,\n",
        "    \"7 day\",\n",
        "    start_date=darts_ts_test.start_time(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5TNCGQS89No-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TNCGQS89No-",
        "outputId": "8b32aeb0-e791-4356-c11a-0e29e9f7c624"
      },
      "outputs": [],
      "source": [
        "nbeats_pred = model_nbeats.predict(n=24*7)\n",
        "nbeats_test = darts_ts_test[0:(24*7)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LB7BEsut-8t8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "LB7BEsut-8t8",
        "outputId": "43cd6e20-dc6c-4fb2-d4aa-0b6c4cc504a2"
      },
      "outputs": [],
      "source": [
        "if slow:\n",
        "  nbeats_pred.plot()\n",
        "  nbeats_test.plot()\n",
        "else:\n",
        "  nbeats_pred.plot()\n",
        "  ts_train[501:(501+24*7)].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Rzr0-7mcaOs",
      "metadata": {
        "id": "8Rzr0-7mcaOs"
      },
      "source": [
        "# NHITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hop2_eMUrVc_",
      "metadata": {
        "id": "Hop2_eMUrVc_"
      },
      "outputs": [],
      "source": [
        "from statsforecast import StatsForecast\n",
        "from ray import tune\n",
        "\n",
        "from neuralforecast.auto import AutoNHITS, AutoLSTM\n",
        "from neuralforecast.core import NeuralForecast\n",
        "\n",
        "from neuralforecast.losses.pytorch import DistributionLoss, MQLoss\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CxZqppS9xj9G",
      "metadata": {
        "id": "CxZqppS9xj9G"
      },
      "outputs": [],
      "source": [
        "ts_nhits = ts.reset_index().rename(columns={\"index\":\"ds\", 0:\"y\"})\n",
        "# NeuralForecast is inherently multivariate. We just assign UID as our unique\n",
        "# id, the exact choice doesn't matter.\n",
        "ts_nhits.insert(column=\"unique_id\", value=\"UID\", loc=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NwxW96CBcbN-",
      "metadata": {
        "id": "NwxW96CBcbN-"
      },
      "outputs": [],
      "source": [
        "StatsForecast.plot(ts_nhits, engine='matplotlib')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_ddZbkmuCNP",
      "metadata": {
        "id": "I_ddZbkmuCNP"
      },
      "source": [
        "Everything's set up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GJZULYgSco-C",
      "metadata": {
        "id": "GJZULYgSco-C"
      },
      "outputs": [],
      "source": [
        "horizon = 24*5 # We run into GPU-RAM limitations for larger horizons\n",
        "\n",
        "\n",
        "# in the tune.choice, one can also specify multiple choices and NeuralForecase\n",
        "# will optimize. That would take too long for us, though.\n",
        "nhits_config = {\n",
        "    \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
        "    \"max_steps\": tune.choice([50]),                                         # Number of SGD steps\n",
        "    \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horiz\n",
        "    \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
        "    \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
        "    \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
        "    \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
        "    \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
        "    \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
        "    \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
        "    \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
        "    \"random_seed\": tune.randint(1, 10),\n",
        "    \"scaler_type\": tune.choice(['robust']),\n",
        "    \"val_check_steps\": tune.choice([100])\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "itH6ZN76hA_w",
      "metadata": {
        "id": "itH6ZN76hA_w"
      },
      "outputs": [],
      "source": [
        "models = [AutoNHITS(h=horizon,\n",
        "                    loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
        "                    config=nhits_config,\n",
        "                    num_samples=3)] # num_samples war 5\n",
        "\n",
        "nf = NeuralForecast(\n",
        "    models=models,\n",
        "    freq=\"1h\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2SE8TknFo23k",
      "metadata": {
        "id": "2SE8TknFo23k"
      },
      "outputs": [],
      "source": [
        "if slow:\n",
        "  nf.fit(df=ts_nhits[1:(len(ts_nhits)-5*24)])\n",
        "else:\n",
        "  # only test on a small subset to make sure the code is working etc.\n",
        "  # NOT for production.\n",
        "  nf.fit(df=ts_nhits[1:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zJNEMVO8o3k0",
      "metadata": {
        "id": "zJNEMVO8o3k0"
      },
      "outputs": [],
      "source": [
        "fcst_df = nf.predict()\n",
        "# in my understanding, the AutoNHITS column will have this information already,\n",
        "# so we just drop this one (differing from the tutorial).\n",
        "fcst_df = fcst_df.drop(\"AutoNHITS-median\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67K4VkAeP8Xi",
      "metadata": {
        "id": "67K4VkAeP8Xi"
      },
      "outputs": [],
      "source": [
        "if slow:\n",
        "  StatsForecast.plot(ts_nhits, fcst_df,engine='matplotlib', level=[80, 90])\n",
        "else:\n",
        "  StatsForecast.plot(ts_nhits[2000-24:(2000+5*24)], fcst_df,engine='matplotlib', level=[80, 90])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dNuFlNPoto6Y",
      "metadata": {
        "id": "dNuFlNPoto6Y"
      },
      "outputs": [],
      "source": [
        "# as an aside, NeuralForecast also supports LSTM, so one could also do a full model comparison as follows:\n",
        "\n",
        "if False:\n",
        "  config_lstm = {\n",
        "      \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n",
        "      \"encoder_hidden_size\": tune.choice([64, 128]),            # Hidden size of LSTM cells\n",
        "      \"encoder_n_layers\": tune.choice([2,4]),                   # Number of layers in LSTM\n",
        "      \"learning_rate\": tune.loguniform(1e-4, 1e-2),             # Initial Learning rate\n",
        "      \"scaler_type\": tune.choice(['robust']),                   # Scaler type\n",
        "      \"max_steps\": tune.choice([500, 1000]),                    # Max number of training iterations\n",
        "      \"batch_size\": tune.choice([1, 4]),                        # Number of series in batch\n",
        "      \"random_seed\": tune.randint(1, 20),                       # Random seed\n",
        "  }\n",
        "\n",
        "  models.append(AutoLSTM(h=48, config=config_lstm, loss=MQLoss(), num_samples=2))\n",
        "# .. then run NeuralForecast as above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MFZgsgWsRyOP",
      "metadata": {
        "id": "MFZgsgWsRyOP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "LvgKpdVV5YoL",
      "metadata": {
        "id": "LvgKpdVV5YoL"
      },
      "source": [
        "# Model Class, Backtesting, Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec4f289",
      "metadata": {
        "id": "dec4f289"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795ce79d",
      "metadata": {
        "id": "795ce79d"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesPredictionModel():\n",
        "    \"\"\"\n",
        "    Time series prediction model implementation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        model_class : class\n",
        "            Choice of regressor\n",
        "        model_params : dict\n",
        "            Definition of model specific tuning parameters\n",
        "\n",
        "    Functions\n",
        "    ----------\n",
        "        init: Initialize model with given parameters\n",
        "        train : Train chosen model\n",
        "        forecast : Apply trained model to prediction period and generate forecast DataFrame\n",
        "    \"\"\"\n",
        "    def __init__(self, model_class, model_params: dict) -> None:\n",
        "        \"\"\"Initialize a new instance of time_series_prediction_model.\"\"\"\n",
        "        self.model_class = model_class\n",
        "        self.model_params = model_params\n",
        "        self.model = None\n",
        "        self.is_univariate = 'endog' in model_class.__init__.__code__.co_varnames\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame = None, y_train: pd.Series = None, train_series: pd.Series = None) -> None:\n",
        "        \"\"\"Train chosen model.\"\"\"\n",
        "\n",
        "        if self.is_univariate:\n",
        "            if train_series is None:\n",
        "                raise ValueError(\"train_series must be provided for univariate models\")\n",
        "            self.train_series = train_series\n",
        "            self.model = self.model_class(endog=self.train_series, **self.model_params)\n",
        "            self.model = self.model.fit()\n",
        "        else:\n",
        "            if X_train is None or y_train is None:\n",
        "                raise ValueError(\"X_train and y_train must be provided for multivariate models\")\n",
        "            self.X_train = X_train\n",
        "            self.y_train = y_train\n",
        "            print(\"hello world\")\n",
        "            self.model = self.model_class(**self.model_params)\n",
        "            self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "    def forecast(self, X_test: pd.DataFrame = None, start_date: str = None, end_date: str = None) -> pd.DataFrame:\n",
        "        \"\"\"Apply trained model to prediction period and generate forecast DataFrame.\"\"\"\n",
        "        if self.is_univariate:\n",
        "            #if steps is None:\n",
        "            if start_date is None or end_date is None:\n",
        "                raise ValueError(\"start_date and end_date must be provided for univariate models\")\n",
        "            #forecast = self.model.predict(start = start_date, end= end_date, typ = 'levels')\n",
        "            # Make predictions\n",
        "            start_date = pd.to_datetime(start_date)\n",
        "            end_date = pd.to_datetime(end_date)\n",
        "            forecast = self.model.predict(start=start_date, end=end_date, typ='levels')\n",
        "\n",
        "            # forecast = self.model.forecast(steps)\n",
        "            forecast_df = pd.DataFrame(forecast, columns=['Forecast'])\n",
        "        else:\n",
        "            if X_test is None:\n",
        "                raise ValueError(\"X_test must be provided for multivariate models\")\n",
        "            self.X_test = X_test\n",
        "            forecast_df = pd.DataFrame(self.model.predict(self.X_test), index=self.X_test.index)\n",
        "            forecast_df.index.name = 'Datum'\n",
        "        return forecast_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3330d3ca",
      "metadata": {
        "id": "3330d3ca"
      },
      "outputs": [],
      "source": [
        "# Backtesting with sliding window\n",
        "\n",
        "def backtesting(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
        "                X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
        "                model: TimeSeriesPredictionModel, prediction_step_size: int=96):\n",
        "    \"\"\"\n",
        "    Perform rolling forecast backtesting for a time series prediction model using\n",
        "    specified train and test datasets, and a given model.\n",
        "\n",
        "    This function splits the test data into multiple windows based on the\n",
        "    prediction_step_size and sequentially forecasts each window. After each\n",
        "    forecasting step, the window of test data used for the current prediction is\n",
        "    added to the training data, and the earliest window of the training data is\n",
        "    removed. The predictions are stored in a DataFrame alongside the original test data values.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Training feature dataset.\n",
        "        y_train (pd.DataFrame): Training target dataset.\n",
        "        X_test (pd.DataFrame): Testing feature dataset.\n",
        "        y_test (pd.DataFrame): Testing target dataset.\n",
        "        model (TimeSeriesPredictionModel): The model used for time series forecasting.\n",
        "        prediction_step_size (int): The number of time steps to predict at each iteration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with two columns 'Original' and 'Predictions',\n",
        "                      containing the actual values from y_test and the predictions\n",
        "                      made by the model, respectively.\n",
        "    \"\"\"\n",
        "\n",
        "    # initializing output df\n",
        "    predictions = pd.DataFrame(index=y_test.index, columns=['Original', 'Predictions'])\n",
        "    predictions['Original'] = y_test\n",
        "\n",
        "    for i in range(0, len(X_test)-prediction_step_size, prediction_step_size):\n",
        "        end_idx = i + prediction_step_size\n",
        "        forecast_index= X_test.iloc[i:end_idx].index\n",
        "\n",
        "        # fit model and predict\n",
        "        model.train(X_train, y_train)\n",
        "        forecast = model.forecast(X_test.iloc[i:end_idx])\n",
        "        predictions.loc[forecast_index, 'Predictions'] = forecast.to_numpy()\n",
        "\n",
        "        print(f'Finished Forecast for {forecast_index[-1].date()}')\n",
        "\n",
        "        # delete old time window from train data\n",
        "        X_train = X_train.drop(X_train.head(prediction_step_size).index)\n",
        "        y_train = y_train.drop(y_train.head(prediction_step_size).index)\n",
        "\n",
        "        # add next time window to train data\n",
        "        X_train = pd.concat([X_train, X_test.iloc[i:end_idx]])\n",
        "        y_train = pd.concat([y_train, y_test.iloc[i:end_idx]])\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac37442",
      "metadata": {
        "id": "7ac37442"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "\n",
        "def evaluation(y_true, y_pred):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate various error metrics to evaluate the accuracy of a regression model.\n",
        "\n",
        "    This function computes the mean absolute error (MAE), mean absolute percentage error (MAPE),\n",
        "    mean squared error (MSE), coefficient of determination (R^2 score), and root mean squared error (RMSE)\n",
        "    between the actual and predicted values.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True values for the target variable.\n",
        "        y_pred (array-like): Predicted values generated by the model.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "               - mae (float): Mean absolute error.\n",
        "               - mape (float): Mean absolute percentage error.\n",
        "               - mse (float): Mean squared error.\n",
        "               - r2 (float): R^2 score, measuring the proportion of variation explained by the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "    return mae, mape, mse, r2, rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "swCOpYEGtg9n",
      "metadata": {
        "id": "swCOpYEGtg9n"
      },
      "source": [
        "# Univariate data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eebmKg4ZsUbh",
      "metadata": {
        "id": "eebmKg4ZsUbh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics import tsaplots\n",
        "from statsmodels.tsa.stattools import acf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZeWORtEKsWho",
      "metadata": {
        "id": "ZeWORtEKsWho"
      },
      "outputs": [],
      "source": [
        "# Autocorrelation plots\n",
        "x_label = 'Lags'\n",
        "y_label = 'Autocorrelation'\n",
        "\n",
        "# Titles for each subplot\n",
        "plot_titles = ['2 Days (48 h)', '2 Weeks (336)', '2 Months (1487 h)', '2 Years (17520 h)']\n",
        "lags = [48, 336, 1487, 17520]\n",
        "\n",
        "# X and Y labels\n",
        "x_label = 'Lags'\n",
        "y_label = 'Autocorrelation'\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "all_autocorr_values = {}\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    tsaplots.plot_acf(ts, ax=ax, lags=lags[i])\n",
        "    ax.set_title(plot_titles[i])\n",
        "    ax.set_xlabel(x_label)\n",
        "    ax.set_ylabel(y_label)\n",
        "\n",
        "    # Compute autocorrelation values\n",
        "    autocorr_values = acf(ts, nlags=lags[i])\n",
        "\n",
        "    # Collect all autocorrelation values\n",
        "    for lag in range(1, len(autocorr_values)):\n",
        "        if lag not in all_autocorr_values:\n",
        "            all_autocorr_values[lag] = []\n",
        "        all_autocorr_values[lag].append(autocorr_values[lag])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Combine all autocorrelation values and find the highest values\n",
        "combined_autocorr_values = {lag: np.mean(values) for lag, values in all_autocorr_values.items()}\n",
        "\n",
        "# Get the highest values\n",
        "sorted_lags = sorted(combined_autocorr_values.items(), key=lambda item: item[1], reverse=True)\n",
        "sorted_lags_filtered = [item for item in sorted_lags if item[0] >= 24]\n",
        "top_5_combined_lags = sorted_lags_filtered[:5]\n",
        "least_3_combined_lags = sorted_lags_filtered[-3:]\n",
        "\n",
        "print(\"Top 5 lags with highest combined autocorrelation values:\")\n",
        "for lag, value in top_5_combined_lags:\n",
        "    print(f\"Lag {lag}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "INDj4Y4xtx6F",
      "metadata": {
        "id": "INDj4Y4xtx6F"
      },
      "outputs": [],
      "source": [
        "# Data preparation for univariate TimeSeriesPredictionModel\n",
        "lags = [24, 48, 72, 96, 120]\n",
        "\n",
        "data = pd.DataFrame(index=ts.index)\n",
        "data['Original'] = ts\n",
        "for lag in lags:\n",
        "    data[f'{lag}_Lag'] = ts.shift(lag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fEqjgN3VuFhO",
      "metadata": {
        "id": "fEqjgN3VuFhO"
      },
      "outputs": [],
      "source": [
        "# Data train-test split\n",
        "train_df = data[:'2022-12-31 23:00:00+00:00']\n",
        "train_df = train_df.dropna()\n",
        "y_train = train_df[['Original']]\n",
        "X_train = train_df.drop(columns=['Original'])\n",
        "\n",
        "valuation_df = data['2023-01-01 00:00:00+00:00':'2023-12-31 23:00:00+00:00']\n",
        "y_valuation = valuation_df[['Original']]\n",
        "X_valuation = valuation_df.drop(columns=['Original'])\n",
        "\n",
        "test_df = data['2024-01-01 00:00:00+00:00':]\n",
        "y_test = test_df[['Original']]\n",
        "X_test = test_df.drop(columns=['Original'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M9jJRdkitLCG",
      "metadata": {
        "id": "M9jJRdkitLCG"
      },
      "source": [
        "# Univariate Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XWbUaUSutSpA",
      "metadata": {
        "id": "XWbUaUSutSpA"
      },
      "source": [
        "## Naive model: Moving Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jFS_6JExtXCn",
      "metadata": {
        "id": "jFS_6JExtXCn"
      },
      "outputs": [],
      "source": [
        "# Moving average model\n",
        "def moving_average(data: pd.DataFrame, window_size: int=3, shift_size: int=24):\n",
        "    moving_avg = data.rolling(window=window_size).mean()\n",
        "    shifted_moving_avg = moving_avg.shift(shift_size)\n",
        "    return(shifted_moving_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QJBGDUFEtYt9",
      "metadata": {
        "id": "QJBGDUFEtYt9"
      },
      "outputs": [],
      "source": [
        "# Plot Naive Model Predictions\n",
        "naive_model = moving_average(ts)\n",
        "\n",
        "test_date_start = '2024-01-01 00:00:00+00:00'\n",
        "test_ts = ts[test_date_start:]\n",
        "naive_model_print = naive_model[test_date_start:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts.index, test_ts, label='Original')\n",
        "plt.plot(naive_model_print.index, naive_model_print, label='Moving average', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Moving Average')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DrIpSLaVtbUz",
      "metadata": {
        "id": "DrIpSLaVtbUz"
      },
      "outputs": [],
      "source": [
        "# Detail Plot Naive Model Predictions\n",
        "naive_model = moving_average(ts)\n",
        "\n",
        "test_date_start = '2024-04-24 00:00:00+00:00'\n",
        "test_ts = ts[test_date_start:]\n",
        "naive_model_print = naive_model[test_date_start:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts.index, test_ts, label='Original')\n",
        "plt.plot(naive_model_print.index, naive_model_print, label='Moving average', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Moving Average')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68oJyge4uKHU",
      "metadata": {
        "id": "68oJyge4uKHU"
      },
      "outputs": [],
      "source": [
        "# Metrics Naive Model\n",
        "mae, mape, mse, r2 = evaluation(test_ts, naive_model_print)\n",
        "\n",
        "print(f'Model: Naive Moving Average \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JAfsHUTOugjB",
      "metadata": {
        "id": "JAfsHUTOugjB"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "snsXcUeVuiSb",
      "metadata": {
        "id": "snsXcUeVuiSb"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lGXokZhWuj4c",
      "metadata": {
        "id": "lGXokZhWuj4c"
      },
      "outputs": [],
      "source": [
        "# Initializing random forest regressor as instance of TimeSeriesPredictionModel\n",
        "rdnf = TimeSeriesPredictionModel(RandomForestRegressor, {'n_estimators': 150, 'criterion': 'squared_error', 'max_depth': 10})\n",
        "# NaNs introduced by slicing\n",
        "X_train = X_train.dropna()\n",
        "y_train = y_train.loc[X_train.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FxCOqkZjulav",
      "metadata": {
        "id": "FxCOqkZjulav"
      },
      "outputs": [],
      "source": [
        "rdnf_pred = backtesting(X_train, y_train, X_test, y_test, rdnf)\n",
        "rdnf_pred = rdnf_pred.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SI_PSByUuzyR",
      "metadata": {
        "id": "SI_PSByUuzyR"
      },
      "outputs": [],
      "source": [
        "# Plot Random Forest Predictions\n",
        "test_date_start = '2024-01-01 00:00+00:00'\n",
        "test_ts = ts[test_date_start:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts.index, test_ts, label='Original')\n",
        "plt.plot(rdnf_pred.index, rdnf_pred['Predictions'], label='Random Forest', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Random Forest')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nnnzoYe9u2D-",
      "metadata": {
        "id": "nnnzoYe9u2D-"
      },
      "outputs": [],
      "source": [
        "# Detail Plot Random Forest Predictions\n",
        "test_date_start = pd.to_datetime('2024-04-24 00:00+00:00')\n",
        "test_date_end = pd.to_datetime('2024-04-30 23:00:00+00:00')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts[test_date_start:test_date_end].index, test_ts[test_date_start:test_date_end], label='Original')\n",
        "plt.plot(rdnf_pred[test_date_start:test_date_end].index, rdnf_pred[test_date_start:test_date_end]['Predictions'], label='Random Forest', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Random Forest')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FR-LHlvpvBdT",
      "metadata": {
        "id": "FR-LHlvpvBdT"
      },
      "outputs": [],
      "source": [
        "mae, mape, mse, r2 = evaluation(rdnf_pred['Original'], rdnf_pred['Predictions'])\n",
        "\n",
        "print(f'Model: Random Forest \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaA_wuUFvDiX",
      "metadata": {
        "id": "eaA_wuUFvDiX"
      },
      "outputs": [],
      "source": [
        "# Metrics comparison random forest default settings v.s. optimized hyperparameters\n",
        "rdn_forest_default = [0.85, 2.54, 0.83]\n",
        "rdn_forest_optimized = [mae, mse, r2]\n",
        "index = ['mae', 'mse',\n",
        "         'r2']\n",
        "df = pd.DataFrame({'Rdn Forest': rdn_forest_default,\n",
        "                   'Rdn Forest optimized': rdn_forest_optimized}, index=index)\n",
        "ax = df.plot.bar(rot=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec90d9e8",
      "metadata": {},
      "source": [
        "# CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ac-9Bku1vLrE",
      "metadata": {
        "id": "Ac-9Bku1vLrE"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Initializing CatBoost regressor as instance of TimeSeriesPredictionModel\n",
        "cboost = TimeSeriesPredictionModel(CatBoostRegressor, {'iterations': 2, 'learning_rate': 0.25, 'depth': 16})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MVhAXViovOUA",
      "metadata": {
        "id": "MVhAXViovOUA"
      },
      "outputs": [],
      "source": [
        "cboost_pred = backtesting(X_train, y_train, X_test, y_test, cboost)\n",
        "cboost_pred = cboost_pred.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nFBg_fSivKXX",
      "metadata": {
        "id": "nFBg_fSivKXX"
      },
      "outputs": [],
      "source": [
        "# Plot CatBoost Predictions\n",
        "test_date_start = pd.to_datetime('2024-01-01 00:00+00:00')\n",
        "test_ts = ts[test_date_start:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts.index, test_ts, label='Original')\n",
        "plt.plot(cboost_pred.index, cboost_pred['Predictions'], label='CatBoost', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('CatBoost')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ouKyKcbCvR1o",
      "metadata": {
        "id": "ouKyKcbCvR1o"
      },
      "outputs": [],
      "source": [
        "# Detail Plot CatBoost Predictions\n",
        "test_date_start = pd.to_datetime('2024-04-24 00:00+00:00')\n",
        "test_date_end = pd.to_datetime('2024-04-30 23:00:00+00:00')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts[test_date_start:test_date_end].index, test_ts[test_date_start:test_date_end], label='Original')\n",
        "plt.plot(cboost_pred[test_date_start:test_date_end].index, cboost_pred[test_date_start:test_date_end]['Predictions'], label='CatBoost', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('CatBoost')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PTDYmzeJvU6c",
      "metadata": {
        "id": "PTDYmzeJvU6c"
      },
      "outputs": [],
      "source": [
        "mae, mape, mse, r2 = evaluation(cboost_pred['Original'], cboost_pred['Predictions'])\n",
        "\n",
        "print(f'Model: CatBoost \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tz7prQAvEMDe",
      "metadata": {
        "id": "Tz7prQAvEMDe"
      },
      "source": [
        "## FFT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mIeW_AchEMw2",
      "metadata": {
        "id": "mIeW_AchEMw2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.models import FFT, AutoARIMA, ExponentialSmoothing, Theta\n",
        "from darts.metrics import mae\n",
        "from darts.utils.missing_values import fill_missing_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_cNs1jnKNiFJ",
      "metadata": {
        "id": "_cNs1jnKNiFJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(19,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Io_i7CHS0Yli",
      "metadata": {
        "id": "Io_i7CHS0Yli"
      },
      "outputs": [],
      "source": [
        "def do_fourier(model, darts_ts_train, darts_ts_test):\n",
        "  model.fit(darts_ts_train)\n",
        "  pred_val = model.predict(len(darts_ts_test))\n",
        "  plt.figure(figsize=(24,4))\n",
        "  #darts_ts_train.plot(label=\"train\")\n",
        "  darts_ts_test.plot(label=\"val\")\n",
        "  print(\"MAE:\", mae(pred_val, darts_ts_test))\n",
        "  return pred_val.plot(label=\"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JngxR4rOIUva",
      "metadata": {
        "id": "JngxR4rOIUva"
      },
      "outputs": [],
      "source": [
        "# baseline FFT model\n",
        "# data is the same as prepared in the chapter on Gaussian Processes.\n",
        "model = FFT(required_matches=set(), nr_freqs_to_keep=None)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s7Y0XIaPR1Cr",
      "metadata": {
        "id": "s7Y0XIaPR1Cr"
      },
      "outputs": [],
      "source": [
        "# interesting: matching day and month makes the result worse whereas it should\n",
        "# make it better!\n",
        "model = FFT(required_matches={\"day\", \"month\"}, nr_freqs_to_keep=None)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mdG0W_fDerZZ",
      "metadata": {
        "id": "mdG0W_fDerZZ"
      },
      "outputs": [],
      "source": [
        "# zoom-in\n",
        "# the plot in the presentation was done on the data on their original resolution\n",
        "model = FFT(nr_freqs_to_keep=5000)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test[:175])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wWoljzoiOfjE",
      "metadata": {
        "id": "wWoljzoiOfjE"
      },
      "outputs": [],
      "source": [
        "import darts.metrics.metrics\n",
        "\n",
        "if slow:\n",
        "  model = FFT()\n",
        "  model.gridsearch(\n",
        "  parameters={\n",
        "        \"nr_freqs_to_keep\": [10, 50, 100, 500, 1000, 2000, 5000, 10000, 15000, 20000]\n",
        "    },\n",
        "    series=darts_ts_train,\n",
        "    val_series=darts_ts_test,\n",
        "    verbose=True,\n",
        "    metric = darts.metrics.metrics.mae\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y0f6sv7uvXhB",
      "metadata": {
        "id": "Y0f6sv7uvXhB"
      },
      "source": [
        "# Optimizer univariate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BUFG3IImva1G",
      "metadata": {
        "id": "BUFG3IImva1G"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def random_search_optimization(model_class, param_grid, train_series=None, X_train=None, y_train=None,\n",
        "                               X_val=None, y_val=None, n_iter=10, scoring_function=None):\n",
        "    \"\"\"\n",
        "    Perform random search optimization on model parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_class : class\n",
        "        Choice of model class (e.g., ARIMA, LinearRegression)\n",
        "    param_grid : dict\n",
        "        Dictionary with parameter names as keys and lists of parameter settings to try as values\n",
        "    train_series : pd.Series, optional\n",
        "        Training data for univariate models\n",
        "    X_train : pd.DataFrame, optional\n",
        "        Training features for multivariate models\n",
        "    y_train : pd.Series, optional\n",
        "        Training target for multivariate models\n",
        "    X_val : pd.DataFrame, optional\n",
        "        Validation features for multivariate models\n",
        "    y_val : pd.Series, optional\n",
        "        Validation target for multivariate models\n",
        "    n_iter : int\n",
        "        Number of parameter settings that are sampled\n",
        "    scoring_function : function\n",
        "        Function to evaluate model performance, should return a single score\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    best_params : dict\n",
        "        Best parameter combination found\n",
        "    best_score : float\n",
        "        Best score obtained\n",
        "    \"\"\"\n",
        "\n",
        "    def sample_params(param_grid):\n",
        "        return {key: random.choice(values) for key, values in param_grid.items()}\n",
        "\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        print(i)\n",
        "        params = sample_params(param_grid)\n",
        "        model = TimeSeriesPredictionModel(model_class, params)\n",
        "\n",
        "        if train_series is not None:\n",
        "            model.train(train_series=train_series)\n",
        "            forecast = model.forecast(steps=len(train_series))\n",
        "            score = scoring_function(train_series[-len(forecast):], forecast)\n",
        "        else:\n",
        "            model.train(X_train=X_train, y_train=y_train)\n",
        "            forecast = model.forecast(X_test=X_val)\n",
        "            score = scoring_function(y_val, forecast)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iN31W8Vfvdn9",
      "metadata": {
        "id": "iN31W8Vfvdn9"
      },
      "outputs": [],
      "source": [
        "# Model Optimization - Example usage\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'criterion': ['squared_error'],\n",
        "    'max_depth': [5, 10, 20]}\n",
        "    # 'iterations': [20, 25, 30],\n",
        "    # 'learning_rate': [0.25, 0.5, 0.75],\n",
        "    # 'depth': [5, 10, 16]}\n",
        "\n",
        "# Example scoring function\n",
        "def my_scoring_function(true, pred):\n",
        "    return -mean_squared_error(true, pred)\n",
        "\n",
        "# Perform random search optimization\n",
        "best_params, best_score = random_search_optimization(\n",
        "    RandomForestRegressor,\n",
        "    param_grid,\n",
        "    X_train= X_train,\n",
        "    y_train= y_train,\n",
        "    X_val = X_valuation,\n",
        "    y_val= y_valuation,\n",
        "    n_iter=10,\n",
        "    scoring_function=my_scoring_function\n",
        ")\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df253d9-7fdc-4f11-8fc5-0ba00695a399",
      "metadata": {
        "id": "4df253d9-7fdc-4f11-8fc5-0ba00695a399"
      },
      "source": [
        "# appendix: Extreme Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1084672e-2558-4252-8beb-49fd3a23e5bf",
      "metadata": {
        "id": "1084672e-2558-4252-8beb-49fd3a23e5bf"
      },
      "outputs": [],
      "source": [
        "from pyextremes import __version__, get_extremes\n",
        "from pyextremes.plotting import plot_extremes\n",
        "from pyextremes import EVA\n",
        "print(\"pyextremes\", __version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a841ad-fcef-4a92-85cb-335410cc0945",
      "metadata": {
        "id": "e9a841ad-fcef-4a92-85cb-335410cc0945"
      },
      "outputs": [],
      "source": [
        "# \"In order for the analysis results to be meaningful, data needs to be pre-processed by the user.\n",
        "# This may include removal of data gaps, detrending, interpolation, removal of outliers, etc.\"\n",
        "# ==> !! Data needs to be detrended! TODO: Does this also imply constant variance?\n",
        "# I assume yes and take solar_ts_series_corrected_variance as in input\n",
        "# TODO: So what exactly does pyextremes expect from the time series?\n",
        "from pyextremes import EVA\n",
        "\n",
        "model = EVA(ts)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1554d6-43ca-41d7-8bf1-63694958fdbb",
      "metadata": {
        "id": "6a1554d6-43ca-41d7-8bf1-63694958fdbb"
      },
      "outputs": [],
      "source": [
        "model.get_extremes(method=\"BM\", block_size=\"365.2425D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96daa35-5785-4a9e-ad45-f2d62b1ab872",
      "metadata": {
        "id": "c96daa35-5785-4a9e-ad45-f2d62b1ab872"
      },
      "outputs": [],
      "source": [
        "model.fit_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414d473d-d9ef-4dea-94b2-5bf1217ef55a",
      "metadata": {
        "id": "414d473d-d9ef-4dea-94b2-5bf1217ef55a"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21bd7c35-a112-4d7d-8704-26bb6c417452",
      "metadata": {
        "id": "21bd7c35-a112-4d7d-8704-26bb6c417452"
      },
      "outputs": [],
      "source": [
        "summary = model.get_summary(\n",
        "    return_period=[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000],\n",
        "    alpha=0.95,\n",
        "    n_samples=1000,\n",
        ")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8262360c-8f72-446b-be03-cce5284b1c44",
      "metadata": {
        "id": "8262360c-8f72-446b-be03-cce5284b1c44"
      },
      "outputs": [],
      "source": [
        "model.plot_diagnostic(alpha=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dYV_0oCYHHMH",
      "metadata": {
        "id": "dYV_0oCYHHMH"
      },
      "source": [
        "The lib can be used to obtain extreme values - either as the extreme values of a period or as all values larger than a threshold - easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30240b44-401b-46be-bec0-64293108636c",
      "metadata": {
        "id": "30240b44-401b-46be-bec0-64293108636c"
      },
      "outputs": [],
      "source": [
        "extremes = get_extremes(\n",
        "    ts=solar_ts_series_corrected_variance,\n",
        "    method=\"BM\",\n",
        "    extremes_type=\"high\",\n",
        "    block_size=\"365.2425D\",\n",
        "    errors=\"raise\",\n",
        "    min_last_block=None,\n",
        ")\n",
        "\n",
        "plot_extremes(\n",
        "    ts=solar_ts_series_corrected_variance,\n",
        "    extremes=extremes,\n",
        "    extremes_method=\"BM\",\n",
        "    extremes_type=\"low\",\n",
        "    block_size=\"365.2425D\",\n",
        ")\n",
        "extremes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275c4bd2-47d0-4c83-b2ee-eb1cc52d7834",
      "metadata": {
        "id": "275c4bd2-47d0-4c83-b2ee-eb1cc52d7834"
      },
      "outputs": [],
      "source": [
        "model.get_extremes(\"POT\", threshold=3, r=\"12H\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84348feb-73e0-4b31-8f50-8797a8cd68b3",
      "metadata": {
        "id": "84348feb-73e0-4b31-8f50-8797a8cd68b3"
      },
      "outputs": [],
      "source": [
        "model.plot_extremes(show_clusters=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dg2DLDikzFgT",
      "metadata": {
        "id": "dg2DLDikzFgT"
      },
      "source": [
        "# discuss: AutoARIMA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ztgWMis8Zeo",
      "metadata": {
        "id": "4ztgWMis8Zeo"
      },
      "outputs": [],
      "source": [
        "# auto arima aic criterion\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "# get best ar ma\n",
        "model = auto_arima(solar_ts_series[1:10000],\n",
        "                   start_p=0, start_q=0,\n",
        "                   max_p=3, max_q=3, m=1,\n",
        "                   start_P=0, seasonal=True,\n",
        "                   d=1, D=1, trace=True,\n",
        "                   error_action='ignore',\n",
        "                   suppress_warnings=True,\n",
        "                   stepwise=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WVR65Hcs5x2w",
      "metadata": {
        "id": "WVR65Hcs5x2w"
      },
      "outputs": [],
      "source": [
        "# acf pacf plot with significNCE LINE\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "plot_acf(solar_ts_series[1:10000], lags=365, alpha=0.05)\n",
        "plot_pacf(solar_ts_series[1:10000], lags=365, alpha=0.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0rnXHxlEytPp",
      "metadata": {
        "id": "0rnXHxlEytPp"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(nsteps=4*24*4)\n",
        "mean_pred, var_pred = pred\n",
        "\n",
        "\n",
        "# Plot the actual data and the forecast for the last 100 observations\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the actual data for the last 100 observations\n",
        "plt.plot(solar_ts_series_corrected_variance[19900:(20000+4*24*4)], label='Actual Data')\n",
        "\n",
        "# Plot the forecasted mean, starting from the last actual value\n",
        "last_observation_index = solar_ts_series_corrected_variance.index[20000]# solar_ts_series_corrected_variance.index[-1]\n",
        "forecast_index = pd.date_range(start=last_observation_index, periods=len(mean_pred) + 1, freq=\"15min\")[1:]\n",
        "plt.plot(forecast_index[-100:], mean_pred[-100:], label='Forecasted Mean', color='red')\n",
        "plt.plot(forecast_index, mean_pred, label='Forecasted Mean', color='red')\n",
        "\n",
        "\n",
        "# Fill between upper and lower bounds of the forecasted variance\n",
        "plt.fill_between(forecast_index,\n",
        "                 mean_pred - np.sqrt(var_pred),\n",
        "                 mean_pred + np.sqrt(var_pred),\n",
        "                 color='red', alpha=0.2, label='Forecasted Variance')\n",
        "\n",
        "plt.title('ARMA-GARCH Forecast (Last 100 Observations)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Returns')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_fQm6m_Oii_V",
        "U8SsNwEvipI0",
        "zr7ckL8uw3Ub",
        "3c81ebe2-0a6e-4515-82d4-70eefb852582",
        "aMGcWv1xwqWF",
        "kywfuwuJHkso",
        "LvgKpdVV5YoL",
        "swCOpYEGtg9n",
        "XWbUaUSutSpA",
        "JAfsHUTOugjB",
        "Tz7prQAvEMDe",
        "Y0f6sv7uvXhB",
        "4df253d9-7fdc-4f11-8fc5-0ba00695a399",
        "dg2DLDikzFgT"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "019a2b2407c44e698ca2a151c50b7228": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ea67f382ee4cec85f57d91abebcf8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce5f0d9e6784d21a28f24ca525e54ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c22c2a62565464da2e894e12d1e0366",
              "IPY_MODEL_21e9ada380294ce1a67591120c498299",
              "IPY_MODEL_32d505faf0f4482894a9b509e77b8c7b"
            ],
            "layout": "IPY_MODEL_276a62e739b146379c0175483ace9961"
          }
        },
        "21e9ada380294ce1a67591120c498299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ea67f382ee4cec85f57d91abebcf8c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b0688e0fa6a4d6cbc57ed33d95cfd99",
            "value": 1
          }
        },
        "276a62e739b146379c0175483ace9961": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2c22c2a62565464da2e894e12d1e0366": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019a2b2407c44e698ca2a151c50b7228",
            "placeholder": "​",
            "style": "IPY_MODEL_cfe5f80988e445aab74b86d439fc92d3",
            "value": "Epoch 26: 100%"
          }
        },
        "32d505faf0f4482894a9b509e77b8c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbfef429daf44f94b4a4dd07ea6d6058",
            "placeholder": "​",
            "style": "IPY_MODEL_bba4155f11b441fe9b5fe53671e3e466",
            "value": " 1/1 [00:01&lt;00:00,  1.68s/it, train_loss=0.00469, val_loss=0.00637]"
          }
        },
        "6b0688e0fa6a4d6cbc57ed33d95cfd99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bba4155f11b441fe9b5fe53671e3e466": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbfef429daf44f94b4a4dd07ea6d6058": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfe5f80988e445aab74b86d439fc92d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
