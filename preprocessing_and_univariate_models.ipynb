{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train_ts.csv', 'validation': 'val_ts.csv', 'test': 'test_ts.csv'}\n",
    "df_train = pd.read_csv(\"hf://datasets/Creatorin/solarpower/\" + splits[\"train\"])\n",
    "df_validation = pd.read_csv(\"hf://datasets/Creatorin/solarpower/\" + splits[\"validation\"])\n",
    "df_test = pd.read_csv(\"hf://datasets/Creatorin/solarpower/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_validation, df_test])\n",
    "ts = df[\"Leistung\"]\n",
    "ts.index = pd.to_datetime(ts.index, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation\n",
    "ts = ts.interpolate(method=\"time\")\n",
    "# Only now can we infer a frequency.\n",
    "ts = ts.asfreq(pd.infer_freq(ts.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "avg_train, dev_train = ts.mean(), ts.std()\n",
    "ts_series = (ts - avg_train)/dev_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove trend\n",
    "ts_series = ts_series.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove increasing volatility\n",
    "\n",
    "def remove_volatility(ts):\n",
    "    annual_volatility = ts.groupby(ts.index.year).std()\n",
    "    annual_vol_per_day = ts.index.map(lambda d: annual_volatility.loc[d.year])\n",
    "    ts_corrected_variance = ts/annual_vol_per_day\n",
    "    monthly_mean = ts_corrected_variance.groupby(ts_corrected_variance.index.month).mean()\n",
    "    monthly_mean_per_day = ts_corrected_variance.index.map(lambda d: monthly_mean.loc[d.month])\n",
    "    ts_corrected_variance= ts_corrected_variance - monthly_mean_per_day\n",
    "    ts_corrected_variance= ts_corrected_variance[~np.isnan(ts_corrected_variance)]\n",
    "    return ts_corrected_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = remove_volatility(ts_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class, Backtesting, Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPredictionModel():\n",
    "    \"\"\"\n",
    "    Time series prediction model implementation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_class : class\n",
    "            Choice of regressor\n",
    "        model_params : dict\n",
    "            Definition of model specific tuning parameters\n",
    "    \n",
    "    Functions\n",
    "    ----------\n",
    "        init: Initialize model with given parameters\n",
    "        train : Train chosen model\n",
    "        forecast : Apply trained model to prediction period and generate forecast DataFrame\n",
    "    \"\"\"\n",
    "    def __init__(self, model_class, model_params: dict) -> None:\n",
    "        \"\"\"Initialize a new instance of time_series_prediction_model.\"\"\"\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.model = None\n",
    "        self.is_univariate = 'endog' in model_class.__init__.__code__.co_varnames\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame = None, y_train: pd.Series = None, train_series: pd.Series = None) -> None:\n",
    "        \"\"\"Train chosen model.\"\"\"\n",
    "        if self.is_univariate:\n",
    "            if train_series is None:\n",
    "                raise ValueError(\"train_series must be provided for univariate models\")\n",
    "            self.train_series = train_series\n",
    "            self.model = self.model_class(endog=self.train_series, **self.model_params)\n",
    "            self.model = self.model.fit()\n",
    "        else:\n",
    "            if X_train is None or y_train is None:\n",
    "                raise ValueError(\"X_train and y_train must be provided for multivariate models\")\n",
    "            self.X_train = X_train\n",
    "            self.y_train = y_train\n",
    "            self.model = self.model_class(**self.model_params)\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def forecast(self, X_test: pd.DataFrame = None, start_date: str = None, end_date: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Apply trained model to prediction period and generate forecast DataFrame.\"\"\"\n",
    "        if self.is_univariate:\n",
    "            #if steps is None:\n",
    "            if start_date is None or end_date is None:\n",
    "                raise ValueError(\"start_date and end_date must be provided for univariate models\")\n",
    "            #forecast = self.model.predict(start = start_date, end= end_date, typ = 'levels')\n",
    "            # Make predictions\n",
    "            start_date = pd.to_datetime(start_date)\n",
    "            end_date = pd.to_datetime(end_date)\n",
    "            forecast = self.model.predict(start=start_date, end=end_date, typ='levels')\n",
    "\n",
    "            # forecast = self.model.forecast(steps)\n",
    "            forecast_df = pd.DataFrame(forecast, columns=['Forecast'])\n",
    "        else:\n",
    "            if X_test is None:\n",
    "                raise ValueError(\"X_test must be provided for multivariate models\")\n",
    "            self.X_test = X_test\n",
    "            forecast_df = pd.DataFrame(self.model.predict(self.X_test), index=self.X_test.index)\n",
    "            forecast_df.index.name = 'Datum'\n",
    "        return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting with sliding window\n",
    "\n",
    "def backtesting(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "                X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
    "                model: TimeSeriesPredictionModel, prediction_step_size: int=96):\n",
    "    \"\"\"\n",
    "    Perform rolling forecast backtesting for a time series prediction model using\n",
    "    specified train and test datasets, and a given model.\n",
    "\n",
    "    This function splits the test data into multiple windows based on the\n",
    "    prediction_step_size and sequentially forecasts each window. After each\n",
    "    forecasting step, the window of test data used for the current prediction is\n",
    "    added to the training data, and the earliest window of the training data is\n",
    "    removed. The predictions are stored in a DataFrame alongside the original test data values.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature dataset.\n",
    "        y_train (pd.DataFrame): Training target dataset.\n",
    "        X_test (pd.DataFrame): Testing feature dataset.\n",
    "        y_test (pd.DataFrame): Testing target dataset.\n",
    "        model (TimeSeriesPredictionModel): The model used for time series forecasting.\n",
    "        prediction_step_size (int): The number of time steps to predict at each iteration.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with two columns 'Original' and 'Predictions',\n",
    "                      containing the actual values from y_test and the predictions\n",
    "                      made by the model, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing output df\n",
    "    predictions = pd.DataFrame(index=y_test.index, columns=['Original', 'Predictions'])\n",
    "    predictions['Original'] = y_test\n",
    "\n",
    "    for i in range(0, len(X_test)-prediction_step_size, prediction_step_size):\n",
    "        end_idx = i + prediction_step_size\n",
    "        forecast_index= X_test.iloc[i:end_idx].index\n",
    "        \n",
    "        # fit model and predict\n",
    "        model.train(X_train, y_train)\n",
    "        forecast = model.forecast(X_test.iloc[i:end_idx])\n",
    "        predictions.loc[forecast_index, 'Predictions'] = forecast.to_numpy()\n",
    "    \n",
    "        print(f'Finished Forecast for {forecast_index[-1].date()}')\n",
    "\n",
    "        # delete old time window from train data\n",
    "        X_train = X_train.drop(X_train.head(prediction_step_size).index)\n",
    "        y_train = y_train.drop(y_train.head(prediction_step_size).index)\n",
    "\n",
    "        # add next time window to train data\n",
    "        X_train = pd.concat([X_train, X_test.iloc[i:end_idx]])\n",
    "        y_train = pd.concat([y_train, y_test.iloc[i:end_idx]])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate various error metrics to evaluate the accuracy of a regression model.\n",
    "\n",
    "    This function computes the mean absolute error (MAE), mean absolute percentage error (MAPE),\n",
    "    mean squared error (MSE), coefficient of determination (R^2 score), and root mean squared error (RMSE)\n",
    "    between the actual and predicted values.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True values for the target variable.\n",
    "        y_pred (array-like): Predicted values generated by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - mae (float): Mean absolute error.\n",
    "               - mape (float): Mean absolute percentage error.\n",
    "               - mse (float): Mean squared error.\n",
    "               - r2 (float): R^2 score, measuring the proportion of variation explained by the model.\n",
    "               - rmse (float): Root mean squared error.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    return mae, mape, mse, r2, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plots\n",
    "x_label = 'Lags'\n",
    "y_label = 'Autocorrelation'\n",
    "\n",
    "# Titles for each subplot\n",
    "plot_titles = ['2 Days (48 h)', '2 Weeks (336)', '2 Months (1487 h)', '2 Years (17520 h)']\n",
    "lags = [48, 336, 1487, 17520]\n",
    "\n",
    "# X and Y labels\n",
    "x_label = 'Lags'\n",
    "y_label = 'Autocorrelation'\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "all_autocorr_values = {}\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    tsaplots.plot_acf(ts, ax=ax, lags=lags[i])\n",
    "    ax.set_title(plot_titles[i])\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "    # Compute autocorrelation values\n",
    "    autocorr_values = acf(ts, nlags=lags[i])\n",
    "    \n",
    "    # Collect all autocorrelation values\n",
    "    for lag in range(1, len(autocorr_values)):\n",
    "        if lag not in all_autocorr_values:\n",
    "            all_autocorr_values[lag] = []\n",
    "        all_autocorr_values[lag].append(autocorr_values[lag])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combine all autocorrelation values and find the highest values\n",
    "combined_autocorr_values = {lag: np.mean(values) for lag, values in all_autocorr_values.items()}\n",
    "\n",
    "# Get the highest values\n",
    "sorted_lags = sorted(combined_autocorr_values.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_lags_filtered = [item for item in sorted_lags if item[0] >= 24]\n",
    "top_5_combined_lags = sorted_lags_filtered[:5]\n",
    "least_3_combined_lags = sorted_lags_filtered[-3:]\n",
    "\n",
    "print(\"Top 5 lags with highest combined autocorrelation values:\")\n",
    "for lag, value in top_5_combined_lags:\n",
    "    print(f\"Lag {lag}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for univariate TimeSeriesPredictionModel\n",
    "lags = [24, 48, 72, 96, 120]\n",
    "\n",
    "data = pd.DataFrame(index=ts.index)\n",
    "data['Original'] = ts\n",
    "for lag in lags: \n",
    "    data[f'{lag}_Lag'] = ts.shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data train-test split\n",
    "train_df = data[:'2022-12-31 23:00+00:00']\n",
    "y_train = train_df[['Original']]\n",
    "X_train = train_df.drop(columns=['Original'])\n",
    "\n",
    "valuation_df = data['2023-01-01 00:00+00:00':'2023-12-31 23:00+00:00']\n",
    "y_valuation = valuation_df[['Original']]\n",
    "X_valuation = valuation_df.drop(columns=['Original'])\n",
    "\n",
    "test_df = data['2024-01-01 00:00+00:00':]\n",
    "y_test = test_df[['Original']]\n",
    "X_test = test_df.drop(columns=['Original'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model: Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average model\n",
    "def moving_average(data: pd.DataFrame, window_size: int=3, shift_size: int=24):\n",
    "    moving_avg = data.rolling(window=window_size).mean()\n",
    "    shifted_moving_avg = moving_avg.shift(shift_size)\n",
    "    return(shifted_moving_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Naive Model Predictions\n",
    "naive_model = moving_average(ts)\n",
    "\n",
    "test_date_start = '2024-01-01 00:00+00:00'\n",
    "test_ts = ts[test_date_start:]\n",
    "naive_model_print = naive_model[test_date_start:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts.index, test_ts, label='Original')\n",
    "plt.plot(naive_model_print.index, naive_model_print, label='Moving average', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Moving Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detail Plot Naive Model Predictions\n",
    "naive_model = moving_average(ts)\n",
    "\n",
    "test_date_start = '2024-04-24 00:00+00:00'\n",
    "test_ts = ts[test_date_start:]\n",
    "naive_model_print = naive_model[test_date_start:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts.index, test_ts, label='Original')\n",
    "plt.plot(naive_model_print.index, naive_model_print, label='Moving average', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Moving Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Naive Model\n",
    "mae, mape, mse, r2, rmse = evaluation(test_ts, naive_model_print)\n",
    "\n",
    "print(f'Model: Naive Moving Average \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2} \\n Root mean squared error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing random forest regressor as instance of TimeSeriesPredictionModel\n",
    "rdnf = TimeSeriesPredictionModel(RandomForestRegressor, {'n_estimators': 150, 'criterion': 'squared_error', 'max_depth': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdnf_pred = backtesting(X_train, y_train, X_test, y_test, rdnf)\n",
    "rdnf_pred = rdnf_pred.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Forest Predictions\n",
    "test_date_start = '2024-01-01 00:00+00:00'\n",
    "test_ts = ts[test_date_start:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts.index, test_ts, label='Original')\n",
    "plt.plot(rdnf_pred.index, rdnf_pred['Predictions'], label='Random Forest', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Random Forest')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detail Plot Random Forest Predictions\n",
    "test_date_start = '2024-04-24 00:00+00:00'\n",
    "test_date_end = '2024-04-30 23:00:00+00:00'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts[test_date_start:test_date_end].index, test_ts[test_date_start:test_date_end], label='Original')\n",
    "plt.plot(rdnf_pred[test_date_start:test_date_end].index, rdnf_pred[test_date_start:test_date_end]['Predictions'], label='Random Forest', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Random Forest')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mape, mse, r2, rmse = evaluation(rdnf_pred['Original'], rdnf_pred['Predictions'])\n",
    "\n",
    "print(f'Model: Random Forest \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2} \\n Root mean squared error: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics comparison random forest default settings v.s. optimized hyperparameters\n",
    "rdn_forest_default = [0.85, 2.54, 0.83, 1.59]\n",
    "rdn_forest_optimized = [mae, mse, r2, rmse]\n",
    "index = ['mae', 'mse',\n",
    "         'r2', 'rmse']\n",
    "df = pd.DataFrame({'Rdn Forest': rdn_forest_default,\n",
    "                   'Rdn Forest optimized': rdn_forest_optimized}, index=index)\n",
    "ax = df.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Initializing CatBoost regressor as instance of TimeSeriesPredictionModel\n",
    "cboost = TimeSeriesPredictionModel(CatBoostRegressor, {'iterations': 20, 'learning_rate': 0.25, 'depth': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cboost_pred = backtesting(X_train, y_train, X_test, y_test, cboost)\n",
    "cboost_pred = cboost_pred.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_start = '2024-01-01 00:00+00:00'\n",
    "test_ts = ts[test_date_start:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts.index, test_ts, label='Original')\n",
    "plt.plot(cboost_pred.index, rdnf_pred['Predictions'], label='CatBoost', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('CatBoost')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_start = '2024-04-27 00:00+00:00'\n",
    "test_date_end = '2024-05-03 23:00:00+00:00'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts[test_date_start:test_date_end].index, test_ts[test_date_start:test_date_end], label='Original')\n",
    "plt.plot(cboost_pred[test_date_start:test_date_end].index, cboost_pred[test_date_start:test_date_end]['Predictions'], label='CatBoost', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('CatBoost')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mape, mse, r2, rmse = evaluation(cboost_pred['Original'], cboost_pred['Predictions'])\n",
    "\n",
    "print(f'Model: CatBoost \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2} \\n Root mean squared error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Optimizer univariate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def random_search_optimization(model_class, param_grid, train_series=None, X_train=None, y_train=None, \n",
    "                               X_val=None, y_val=None, n_iter=10, scoring_function=None):\n",
    "    \"\"\"\n",
    "    Perform random search optimization on model parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class : class\n",
    "        Choice of model class (e.g., ARIMA, LinearRegression)\n",
    "    param_grid : dict\n",
    "        Dictionary with parameter names as keys and lists of parameter settings to try as values\n",
    "    train_series : pd.Series, optional\n",
    "        Training data for univariate models\n",
    "    X_train : pd.DataFrame, optional\n",
    "        Training features for multivariate models\n",
    "    y_train : pd.Series, optional\n",
    "        Training target for multivariate models\n",
    "    X_val : pd.DataFrame, optional\n",
    "        Validation features for multivariate models\n",
    "    y_val : pd.Series, optional\n",
    "        Validation target for multivariate models\n",
    "    n_iter : int\n",
    "        Number of parameter settings that are sampled\n",
    "    scoring_function : function\n",
    "        Function to evaluate model performance, should return a single score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_params : dict\n",
    "        Best parameter combination found\n",
    "    best_score : float\n",
    "        Best score obtained\n",
    "    \"\"\"\n",
    "    \n",
    "    def sample_params(param_grid):\n",
    "        return {key: random.choice(values) for key, values in param_grid.items()}\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    best_params = None\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        print(i)\n",
    "        params = sample_params(param_grid)\n",
    "        model = TimeSeriesPredictionModel(model_class, params)\n",
    "        \n",
    "        if train_series is not None:\n",
    "            model.train(train_series=train_series)\n",
    "            forecast = model.forecast(steps=len(train_series))\n",
    "            score = scoring_function(train_series[-len(forecast):], forecast)\n",
    "        else:\n",
    "            model.train(X_train=X_train, y_train=y_train)\n",
    "            forecast = model.forecast(X_test=X_val)\n",
    "            score = scoring_function(y_val, forecast)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Optimization - Example usage\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'criterion': ['squared_error'], \n",
    "    'max_depth': [5, 10, 20]}\n",
    "    # 'iterations': [20, 25, 30],\n",
    "    # 'learning_rate': [0.25, 0.5, 0.75],\n",
    "    # 'depth': [5, 10, 16]}\n",
    "\n",
    "# Example scoring function\n",
    "def my_scoring_function(true, pred):\n",
    "    return -mean_squared_error(true, pred)\n",
    "\n",
    "# Perform random search optimization\n",
    "best_params, best_score = random_search_optimization(\n",
    "    RandomForestRegressor, \n",
    "    param_grid, \n",
    "    X_train= X_train, \n",
    "    y_train= y_train, \n",
    "    X_val = X_valuation,\n",
    "    y_val= y_valuation,\n",
    "    n_iter=10, \n",
    "    scoring_function=my_scoring_function\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
