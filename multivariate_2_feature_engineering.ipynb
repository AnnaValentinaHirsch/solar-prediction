{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## If using Colab, run:"
   ],
   "metadata": {
    "id": "ml6_irsbxHsl"
   },
   "id": "ml6_irsbxHsl"
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install numpy matplotlib timeseriesfcst tensorflow pandas datasets scikit-learn"
   ],
   "metadata": {
    "id": "7sg7CY02lgJ-"
   },
   "id": "7sg7CY02lgJ-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install --upgrade timeseriesfcst"
   ],
   "metadata": {
    "id": "56fadmqFo0by"
   },
   "id": "56fadmqFo0by",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "8a206ae351e06039"
   },
   "cell_type": "markdown",
   "source": [
    "# Setup\n"
   ],
   "id": "8a206ae351e06039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T12:59:13.387521Z",
     "start_time": "2024-07-29T12:59:00.641753Z"
    },
    "id": "6224bbe3d9be3ec5"
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timeseriesfcst.preprocessing as tsprep\n",
    "import timeseriesfcst.decomposition as tsdecomp\n",
    "import timeseriesfcst.feature_engineering as tsfeat\n",
    "import timeseriesfcst.feature_selection as tsfs\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import Dataset"
   ],
   "id": "6224bbe3d9be3ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "65a019add2d009b7"
   },
   "cell_type": "markdown",
   "source": [
    "# Load Datset from Huggingface\n",
    "\n"
   ],
   "id": "65a019add2d009b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T12:59:19.516232Z",
     "start_time": "2024-07-29T12:59:13.390305Z"
    },
    "id": "b12ed0c13fb665f"
   },
   "cell_type": "code",
   "source": [
    "REPO_PATH = \"hf://datasets/Creatorin/solarpower/\"\n",
    "splits = {'train': 'train_ts.csv', 'validation': 'val_ts.csv', 'test': 'test_ts.csv'}\n",
    "\n",
    "# Load data\n",
    "train_ts = pd.read_csv(REPO_PATH + splits[\"train\"], index_col=0, date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "val_ts = pd.read_csv(REPO_PATH + splits[\"validation\"], index_col=0, date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "test_ts = pd.read_csv(REPO_PATH + splits[\"test\"], index_col=0, date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Make index datetime\n",
    "train_ts.index = pd.to_datetime(train_ts.index)\n",
    "val_ts.index = pd.to_datetime(val_ts.index)\n",
    "test_ts.index = pd.to_datetime(test_ts.index)\n",
    "\n",
    "# Copy train_ts to undo normalisation later\n",
    "train_ts_copy = train_ts.copy()\n",
    "\n",
    "# Verify Shapes\n",
    "print(f\"Train Shape: {train_ts.shape}, Validation Shape: {val_ts.shape}, Test Shape: {test_ts.shape}\")"
   ],
   "id": "b12ed0c13fb665f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5ee3fb269c656956"
   },
   "cell_type": "markdown",
   "source": [
    "# Preprocess Data\n",
    "## Make Stationary (Remove Trend and Seasonality)"
   ],
   "id": "5ee3fb269c656956"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T12:44:08.652909Z",
     "start_time": "2024-07-29T12:42:41.365223Z"
    },
    "id": "73f3909256d18fec"
   },
   "cell_type": "code",
   "source": [
    "# Make Unit Root Stationary\n",
    "# train_ts = tsdecomp.make_stationary_unitroot(train_ts, val_ts, test_ts)\n",
    "\n",
    "# Check variance stationarity\n",
    "tsdecomp.check_stationarity_variance(train_ts, 24)\n",
    "tsdecomp.check_stationarity_variance(train_ts, 8760)\n",
    "\n",
    "# Check target only\n",
    "tsdecomp.check_stationarity_variance_single(train_ts[\"Leistung\"], 24, plot=True)\n",
    "tsdecomp.check_stationarity_variance_single(train_ts[\"Leistung\"], 8760, plot=True)"
   ],
   "id": "73f3909256d18fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T12:59:19.655240Z",
     "start_time": "2024-07-29T12:59:19.522391Z"
    },
    "id": "33c4b58d1e17e13d"
   },
   "cell_type": "code",
   "source": [
    "# Detrend\n",
    "train_detrend = tsdecomp.detrend_ts(train_ts)\n",
    "val_detrend = tsdecomp.detrend_ts(val_ts)\n",
    "test_detrend = tsdecomp.detrend_ts(test_ts)\n",
    "\n",
    "# Deseasonalise\n",
    "train_deseasonal = tsdecomp.deseasonalise_ts(train_detrend, 8760)\n",
    "val_deseasonal = tsdecomp.deseasonalise_ts(val_detrend, 8760)\n",
    "test_deseasonal = tsdecomp.deseasonalise_ts(test_detrend, 8760)"
   ],
   "id": "33c4b58d1e17e13d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T12:46:55.513541Z",
     "start_time": "2024-07-29T12:46:47.865808Z"
    },
    "id": "57362d5be703e167"
   },
   "cell_type": "code",
   "source": [
    "# Plot Leistung before and after\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_ts[\"Leistung\"], label=\"Original\")\n",
    "plt.plot(train_detrend[\"Leistung\"], label=\"Detrended\")\n",
    "plt.plot(train_deseasonal[\"Leistung\"], label=\"Deseasonalised\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "57362d5be703e167",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2f805e3cabaedfc3"
   },
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n"
   ],
   "id": "2f805e3cabaedfc3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-29T13:00:14.868912Z"
    },
    "id": "57ebf092c0ce08f0"
   },
   "cell_type": "code",
   "source": [
    "# Create Features\n",
    "train_processed, val_processed, test_processed = tsfeat.create_lagged_features(\n",
    "    train_deseasonal, val_deseasonal, test_deseasonal, lags=[1, 2, 3, 4, 5, 6, 12, 24, 48, 168, 8760]\n",
    ")\n",
    "train_processed, val_processed, test_processed = tsfeat.create_rolling_features(\n",
    "    train_processed, val_processed, test_processed, windows=[3, 6, 12, 24, 48, 168, 8760]\n",
    ")\n",
    "train_processed, val_processed, test_processed = tsfeat.create_datetime_features(\n",
    "    train_processed, val_processed, test_processed\n",
    ")\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Train Shape: {train_processed.shape}, Validation Shape: {val_processed.shape}, Test Shape: {test_processed.shape}\")\n",
    "train_processed.head()\n",
    "\n",
    "\n",
    "# Drop first year of train\n",
    "train_processed = train_processed[\"2016-01-01\":]\n",
    "\n",
    "# Handle any remaining missing values\n",
    "for dataset in [train_processed, val_processed, test_processed]:\n",
    "    dataset.fillna(method='ffill', inplace=True)\n",
    "    dataset.fillna(method='bfill', inplace=True)  # In case there are still NaNs at the beginning\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Train Shape: {train_processed.shape}, Validation Shape: {val_processed.shape}, Test Shape: {test_processed.shape}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "for name, dataset in zip(['Train', 'Validation', 'Test'], [train_processed, val_processed, test_processed]):\n",
    "    missing = dataset.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\nMissing values in {name} set:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"\\nNo missing values in {name} set\")"
   ],
   "id": "57ebf092c0ce08f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d09d3220d6adc977"
   },
   "cell_type": "markdown",
   "source": [
    "# Standardise Data"
   ],
   "id": "d09d3220d6adc977"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "id": "e294f2bf352dcbdc"
   },
   "cell_type": "code",
   "source": [
    "# Normalise the data\n",
    "train_processed, val_processed, test_processed = tsprep.normalise_ts(train_processed, val_processed, test_processed)\n",
    "\n",
    "# Check the normalised data mean and std\n",
    "print(f\"Train Mean: {train_processed.mean()}, Train Std: {train_processed.std()}\")"
   ],
   "id": "e294f2bf352dcbdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4eb737b0b3f597e5"
   },
   "cell_type": "markdown",
   "source": [
    "# Feature Selection (Random Forest)\n",
    "\n"
   ],
   "id": "4eb737b0b3f597e5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "id": "77b3969410d12220"
   },
   "cell_type": "code",
   "source": [
    "# Random Forest Feature selection\n",
    "train_selected, selected_features = tsfs.time_series_feature_selection_gpu(train_processed, train_processed[\"Leistung\"], n_features=50, n_estimators=100, max_depth=10, epochs=10)\n",
    "\n",
    "# Apply feature selection to original series, keep target leistung\n",
    "train_rf = train_processed[selected_features]\n",
    "val_rf = val_processed[selected_features]\n",
    "test_rf = test_processed[selected_features]\n",
    "\n",
    "# Reorder Leistung to have Leistustung first\n",
    "train_rf = train_rf[[\"Leistung\"] + [col for col in train_rf.columns if col != \"Leistung\"]]\n",
    "val_rf = val_rf[[\"Leistung\"] + [col for col in val_rf.columns if col != \"Leistung\"]]\n",
    "test_rf = test_rf[[\"Leistung\"] + [col for col in test_rf.columns if col != \"Leistung\"]]\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Train Shape: {train_rf.shape}, Validation Shape: {val_rf.shape}, Test Shape: {test_rf.shape}\")\n",
    "\n",
    "# Print Variables\n",
    "print(train_rf.columns)"
   ],
   "id": "77b3969410d12220",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA"
   ],
   "metadata": {
    "id": "TPf8kUtSrXEb"
   },
   "id": "TPf8kUtSrXEb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Verify shapes\n",
    "print(f\"Train Shape: {train_processed.shape}, Validation Shape: {val_processed.shape}, Test Shape: {test_processed.shape}\")"
   ],
   "id": "f7a29aaa8553aea1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Target\n",
    "target_column = \"Leistung\"\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_processed.drop(columns=[target_column])\n",
    "X_val = val_processed.drop(columns=[target_column])\n",
    "X_test = test_processed.drop(columns=[target_column])\n",
    "\n",
    "y_train = train_processed[target_column]\n",
    "y_val = val_processed[target_column]\n",
    "y_test = test_processed[target_column]\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Transform data\n",
    "train_pca = pca.transform(X_train)\n",
    "val_pca = pca.transform(X_val)\n",
    "test_pca = pca.transform(X_test)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Train Shape: {train_pca.shape}, Validation Shape: {val_pca.shape}, Test Shape: {test_pca.shape}\")\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the number of components selected\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n",
    "\n",
    "# create DataFrames with the PCA results\n",
    "train_pca_df = pd.DataFrame(train_pca, index=X_train.index, columns=[f'PC_{i+1}' for i in range(train_pca.shape[1])])\n",
    "val_pca_df = pd.DataFrame(val_pca, index=X_val.index, columns=[f'PC_{i+1}' for i in range(val_pca.shape[1])])\n",
    "test_pca_df = pd.DataFrame(test_pca, index=X_test.index, columns=[f'PC_{i+1}' for i in range(test_pca.shape[1])])\n",
    "\n",
    "# Add the target variable back to the PCA DataFrames\n",
    "train_pca_df[target_column] = y_train\n",
    "val_pca_df[target_column] = y_val\n",
    "test_pca_df[target_column] = y_test\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Train Shape: {train_pca_df.shape}, Validation Shape: {val_pca_df.shape}, Test Shape: {test_pca_df.shape}\")"
   ],
   "metadata": {
    "id": "TEHfAaBCrYVF"
   },
   "id": "TEHfAaBCrYVF",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "fa8e46b526cbda81"
   },
   "cell_type": "markdown",
   "source": [
    "#  Push Datasets to Hugginface"
   ],
   "id": "fa8e46b526cbda81"
  },
  {
   "metadata": {
    "id": "43448573318c50f4"
   },
   "cell_type": "code",
   "source": [
    "# Feature Selected\n",
    "train_dataset = Dataset.from_pandas(train_rf)\n",
    "val_dataset = Dataset.from_pandas(val_rf)\n",
    "test_dataset = Dataset.from_pandas(test_rf)\n",
    "\n",
    "# train_dataset.push_to_hub(\"Creatorin/solar_selected\", split=\"train\")\n",
    "# val_dataset.push_to_hub(\"Creatorin/solar_selected\", split=\"validation\")\n",
    "# test_dataset.push_to_hub(\"Creatorin/solar_selected\", split=\"test\")"
   ],
   "id": "43448573318c50f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# PCA\n",
    "train_dataset = Dataset.from_pandas(train_pca_df)\n",
    "val_dataset = Dataset.from_pandas(val_pca_df)\n",
    "test_dataset = Dataset.from_pandas(test_pca_df)\n",
    "\n",
    "# train_dataset.push_to_hub(\"Creatorin/solar_pca\", split=\"train\")\n",
    "# val_dataset.push_to_hub(\"Creatorin/solar_pca\", split=\"validation\")\n",
    "# test_dataset.push_to_hub(\"Creatorin/solar_pca\", split=\"test\")"
   ],
   "metadata": {
    "id": "OE3rVNrGruow"
   },
   "id": "OE3rVNrGruow",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "bd19ba4e9d89cc5a"
   },
   "cell_type": "markdown",
   "source": [],
   "id": "bd19ba4e9d89cc5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
